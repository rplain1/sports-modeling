---
format:
  gfm:
    preview-mode: raw
---

```{python}
import numpy as np
import pandas as pd
import pymc as pm
import xarray as xr
import arviz as az
from arviz.stats.stats import hdi
from scipy import stats as stats
from matplotlib import pyplot as plt
import duckdb
import polars as pl
from plotnine import ggplot, aes, geom_density

rng = 527
```

### Load public data

```{python}
def get_data(year):
    qbs = (
        pl.read_parquet(
            f"https://github.com/nflverse/nflverse-data/releases/download/pbp/play_by_play_{year}.parquet"
        )
        .filter(
            (pl.col("wp").is_between(0.1, 0.9))
            & (pl.col("penalty") == 0)
            & (pl.col("qb_dropback") == 1)
            & (pl.col("season_type") == "REG")
        )
        .with_columns(qb_dropbacks=pl.col("qb_dropback").sum().over(["passer_id"]))
        #.filter(pl.col("qb_dropbacks") > 1)
        #.select(["passer_id", "passer", "posteam", "complete_pass", "qb_epa", "qb_dropbacks"])
    )

    return qbs

qbs = pl.concat([get_data(year) for year in range(2021, 2025)])

qb_ids = qbs["passer", "passer_id"].unique()

qbs.head()
```

### Model componets

```{python}
ggplot(qbs, aes("qb_epa")) + geom_density()
print(f"Mean QB EPA/play: {qbs["qb_epa"].mean():.2f}")
print(f"Mode QB EPA/play: {qbs["qb_epa"].round(1).mode()[0]:.2f}")
print(f"Lower 2.5% QB EPA/play: {qbs["qb_epa"].quantile(0.025):.2f}")
print(f"Upper 97.5% QB EPA/play: {qbs["qb_epa"].quantile(0.975):.2f}")
```


```{python}
# categorical to int indexes for pymc
PASSER_IDX, PASSER_ID = pd.factorize(qbs["passer_id"].to_pandas(), sort=True)
N_PASSERS = qbs["passer_id"].n_unique()

```

### Configure Model

```{python}

coords = {"passer_id": PASSER_ID, "obs_id": np.arange(len(PASSER_IDX))}

with pm.Model(coords=coords) as model:

    # data
    passer_idx_data = pm.Data("passer_idx", PASSER_IDX, dims='obs_id')
    y_data = pm.Data("y_data", qbs["qb_epa"], dims='obs_id')

    # parameters
    sigma = pm.HalfNormal("sigma", sigma=0.5)
    sigma_passer = pm.HalfNormal("sigma_passer", sigma=0.5)
    passer_offset = pm.StudentT(
        "passer_offset",
        nu=3,
        mu=-0.2,
        sigma=1,
        dims="passer_id",
    )

    # generated quantities
    passer_effect = pm.Deterministic(
        "passer_effect", passer_offset * sigma_passer, dims="passer_id"
    )
    mu = passer_effect[passer_idx_data]

    # Likelihood
    y = pm.Normal(
        "y",
        mu=mu,
        sigma=sigma,
        observed=y_data,
        shape=passer_idx_data.shape,
        dims="obs_id",
    )

```

### Prior predictive checks

```{python}
with model:
    idata = pm.sample_prior_predictive()
```

```{python}
def plot_priors(idata: az.InferenceData, y="y"):
    assert "prior_predictive" in idata, "`prior_predictive` unavailable"
    az.plot_ppc(idata, group="prior", num_pp_samples=100)

    y_prior = idata.prior_predictive[y]

    prior_median = y_prior.median().values
    prior_hdi = hdi(y_prior, hdi_prob=0.95)
    hdi_lower = prior_hdi[y].sel(hdi="lower").min().item()
    hdi_upper = prior_hdi[y].sel(hdi="higher").max().item()

    print(
        f"""
    Prior median:      {prior_median:.2f}
    Prior lower 2.5%:  {hdi_lower:.2f}
    Prior upper 97.5%:  {hdi_upper:.2f}
    """
    )


plot_priors(idata)
```

### Fit model


```{python}
with model:
    idata = pm.sample(random_seed=rng, nuts={"target_accept": 0.99})

```


```{python}
az.summary(idata)

az.plot_trace(
    idata,
    compact=True,
    var_names=["passer_effect", "passer_offset", "sigma_passer", "sigma"],
)
```


```{python}
df_posterior = idata.posterior.to_dataframe().reset_index().merge(qb_ids.to_pandas(), on=['passer_id'], how='left')
```

```{python}

_qbs = ["B.Nix", "J.Daniels", "C.Williams", "M.Penix"]
_qbs = ['P.Mahomes', 'C.Williams', 'J.Daniels', 'M.Penix', 'Z.Wilson']


df_passer_effect = df_posterior[df_posterior['passer'].isin(_qbs)]

(
    ggplot(df_passer_effect, aes("passer_effect", fill="factor(passer)"))
    + geom_density(alpha=0.2)
)

```


```{python}
pl.from_pandas(df_passer_effect).group_by(["passer"]).agg(
    median=pl.col("passer_effect").median(),
    mean=pl.col("passer_effect").mean(),
    sd=pl.col("passer_effect").std(),
    lower=pl.col("passer_effect").quantile(0.075),
    upper=pl.col("passer_effect").quantile(0.975),
).sort("mean", descending=True)
```


```{python}
with model:
    pm.sample_posterior_predictive(idata, extend_inferencedata=True)
```

```{python}
az.plot_ppc(idata, group='posterior', num_pp_samples=100)
```


```{python}

_qbs = (
    qbs.with_columns(passer_idx=PASSER_IDX)
    .filter(pl.col("passer").is_in(["J.Allen", "C.Williams"]))[
        ["passer", "passer_id", "passer_idx"]
    ]
    .unique()
    .to_pandas()
)

_passers = ["00-0039910", "00-0039918"]

with model:
    pm.set_data(
        {"passer_idx": _qbs["passer_idx"]},
        coords={"passer_id": _qbs["passer_id"], "obs_id": np.arange(_qbs.shape[0])},
    )

    pp = pm.sample_posterior_predictive(
        idata,
        predictions=True,
        var_names=["y"],
        extend_inferencedata=False,
    )

df_post_preds = pp.predictions["y"].assign_coords(
    passer=("obs_id", _qbs['passer'])
).to_dataframe().reset_index()


```


```{python}
(ggplot(df_post_preds, aes('y', fill='factor(passer)')) + geom_density(alpha = 0.2))
```
